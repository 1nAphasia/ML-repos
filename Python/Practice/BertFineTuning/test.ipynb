{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76a415d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms  # 核心数据集和预处理模块\n",
    "from torch.utils.data import DataLoader  # 数据加载器\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e920c254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0976,\n",
      "           0.6959,  2.5542,  2.8215,  2.8088,  1.5614, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.3633,\n",
      "           2.8088,  2.8088,  2.8088,  2.8088,  2.7578,  0.9250, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.8796,  2.7578,\n",
      "           2.8088,  2.8088,  2.8088,  2.8088,  2.8088,  0.9886, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.5924,  2.7197,\n",
      "           0.8741,  0.0849,  1.6378,  2.8088,  2.8088,  0.9886, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.0849,  2.6815,  1.8287,\n",
      "          -0.4242, -0.3988,  1.9814,  2.8088,  2.8088,  0.9886, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.6250,  2.8088,  2.6560,\n",
      "           0.0849,  1.1159,  2.8088,  2.8088,  2.8088,  1.3832, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.2432,  2.8088,  2.8088,\n",
      "           2.8088,  2.8088,  2.8088,  2.8088,  2.8088,  0.9886, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.0806,  2.6687,  2.8088,\n",
      "           2.8088,  2.8088,  2.8088,  2.8088,  2.8088,  0.9886, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.1032,  2.0578,\n",
      "           2.5797,  0.8486,  2.4906,  2.8088,  2.3124, -0.2333, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.1442,  1.3959,  2.8088,  2.8088,  0.7595, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.5940,  2.6815,  2.8088,  1.8160, -0.3988, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.2206,\n",
      "           2.2869,  2.8088,  2.6942,  0.1995, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1442,  2.5542,\n",
      "           2.8088,  2.7833,  0.7850, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  1.8287,  2.8088,\n",
      "           2.8088,  0.6831, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.4596,  2.7960,  2.3505,\n",
      "           0.0722, -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.3988,  1.0268,  2.7960,  2.6433,  0.3904,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.4540,  2.8088,  2.8088,  0.5177, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.1824,  2.6433,  2.8088,  1.4214, -0.3733, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  0.7850,  2.8088,  1.5232, -0.4115, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  1.7269,  2.3251, -0.1187, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), 9)\n"
     ]
    }
   ],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data',         # 数据存储路径\n",
    "    train=True,            # 训练集标记\n",
    "    download=True,         # 如果本地不存在则下载\n",
    "    transform=transform    # 应用预处理\n",
    ")\n",
    "\n",
    "# 测试集\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,           # 测试集标记\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "train_loader=DataLoader(batch_size=batch_size,shuffle=True,dataset=train_dataset)\n",
    "test_loader=DataLoader(batch_size=batch_size,dataset=test_dataset)\n",
    "\n",
    "print(test_dataset[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e37534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNmodel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNmodel,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2=nn.Conv2d(32,64,3,1,1)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.fc=nn.Linear(64*7*7,128)\n",
    "        self.out=nn.Linear(128,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,64*7*7)\n",
    "        x=F.relu(self.fc(x))\n",
    "        x=self.out(x)\n",
    "        return x\n",
    "    \n",
    "def train(model,device,train_loader,optimizer,epoch):\n",
    "    model.train()\n",
    "    for batch_idx,(data,target) in enumerate(train_loader):\n",
    "        data,target=data.to(device),target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output=model(data)\n",
    "        loss=F.cross_entropy(output,target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        if (batch_idx%100)==0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
    "\n",
    "def test(model,device,test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target=data.to(device),target.to(device)\n",
    "            output=model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # 计算损失\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 获取最大概率的索引\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf5e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000] Loss: 2.310168\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.161464\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.188618\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.206894\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.040116\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.010934\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.048808\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.491910\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.131577\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.095757\n",
      "\n",
      "Test set: Average loss: 0.0511, Accuracy: 9830/10000 (98.30%)\n",
      "\n",
      "Train Epoch: 2 [0/60000] Loss: 0.008132\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.069837\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.006585\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.072716\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.038847\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.157350\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.018967\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.107969\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.003945\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.024176\n",
      "\n",
      "Test set: Average loss: 0.0333, Accuracy: 9889/10000 (98.89%)\n",
      "\n",
      "Train Epoch: 3 [0/60000] Loss: 0.016814\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.003288\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.002043\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.003951\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.003409\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.058854\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.018982\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.002580\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.010129\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.035767\n",
      "\n",
      "Test set: Average loss: 0.0317, Accuracy: 9892/10000 (98.92%)\n",
      "\n",
      "Train Epoch: 4 [0/60000] Loss: 0.002492\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.002013\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.000148\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.010213\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.021693\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.053705\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.007342\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.046158\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.058339\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.093479\n",
      "\n",
      "Test set: Average loss: 0.0335, Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000] Loss: 0.003285\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.003898\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.000818\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.003329\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.012385\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.020440\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.000580\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.018468\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.034856\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.087806\n",
      "\n",
      "Test set: Average loss: 0.0355, Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000] Loss: 0.026539\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.000586\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.007095\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.000377\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.000200\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.001619\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.004465\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.002905\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.002244\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.002158\n",
      "\n",
      "Test set: Average loss: 0.0392, Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Train Epoch: 7 [0/60000] Loss: 0.002162\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.001303\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.024353\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.000585\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.004241\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.000088\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.005674\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.006212\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.000462\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.002445\n",
      "\n",
      "Test set: Average loss: 0.0285, Accuracy: 9919/10000 (99.19%)\n",
      "\n",
      "Train Epoch: 8 [0/60000] Loss: 0.000031\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.000135\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.004343\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.015553\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.000245\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.011080\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.008890\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.000291\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.029177\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.000336\n",
      "\n",
      "Test set: Average loss: 0.0295, Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Train Epoch: 9 [0/60000] Loss: 0.000432\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.000286\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.000030\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.000240\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.000743\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.041883\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.032108\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.008989\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.002116\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.054460\n",
      "\n",
      "Test set: Average loss: 0.0498, Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Train Epoch: 10 [0/60000] Loss: 0.005750\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.020910\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.001267\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.000099\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.000211\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.012535\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.000149\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.000016\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.002119\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.000715\n",
      "\n",
      "Test set: Average loss: 0.0511, Accuracy: 9876/10000 (98.76%)\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, int)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, int)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     train(CNNmodel, device, train_loader, optimizer, epoch)\n\u001b[32m     12\u001b[39m     test(CNNmodel,device,test_loader)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mCNNmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m999\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79237\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79237\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mCNNmodel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     x=\u001b[38;5;28mself\u001b[39m.pool(F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     12\u001b[39m     x=\u001b[38;5;28mself\u001b[39m.pool(F.relu(\u001b[38;5;28mself\u001b[39m.conv2(x)))\n\u001b[32m     13\u001b[39m     x=x.view(-\u001b[32m1\u001b[39m,\u001b[32m64\u001b[39m*\u001b[32m7\u001b[39m*\u001b[32m7\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79237\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79237\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79237\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79237\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: conv2d() received an invalid combination of arguments - got (tuple, Parameter, Parameter, tuple, tuple, tuple, int), but expected one of:\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, tuple of ints padding = 0, tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, int)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n * (Tensor input, Tensor weight, Tensor bias = None, tuple of ints stride = 1, str padding = \"valid\", tuple of ints dilation = 1, int groups = 1)\n      didn't match because some of the arguments have invalid types: (!tuple of (Tensor, int)!, !Parameter!, !Parameter!, !tuple of (int, int)!, !tuple of (int, int)!, !tuple of (int, int)!, !int!)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 初始化模型\n",
    "CNNmodel = CNNmodel().to(device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(CNNmodel.parameters(), lr=0.001)\n",
    "\n",
    "# 训练和测试模型\n",
    "for epoch in range(1, 11):\n",
    "    train(CNNmodel, device, train_loader, optimizer, epoch)\n",
    "    test(CNNmodel,device,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12026944",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m (data,target)=\u001b[43mtest_dataset\u001b[49m[\u001b[32m999\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(CNNmodel(data))\n",
      "\u001b[31mNameError\u001b[39m: name 'test_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "(data,target)=test_dataset[999]\n",
    "print(CNNmodel(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
